{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/collin/.local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from models import BachNet\n",
    "from models import ChopinNet\n",
    "\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from heapq import heappop as pop\n",
    "from heapq import heappush as push\n",
    "from utils import graph_utils\n",
    "from utils import display_utils\n",
    "from utils import prediction_utils\n",
    "from utils import preprocessing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"data/train\"\n",
    "test_path = \"data/test\"\n",
    "\n",
    "input_path = \"input\"\n",
    "output_path = \"output\"\n",
    "\n",
    "gt_tag = \"gt\"\n",
    "\n",
    "receptive_field_shape = (12, 12)\n",
    "n_epochs = 5\n",
    "save_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bach = BachNet.BachNet()\n",
    "bach.build(receptive_field_shape, 1)\n",
    "bach.load_model('models/saved_models/Bach/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image:  test_gt\n",
      "Loading image:  test\n",
      "Loading image:  test\n"
     ]
    }
   ],
   "source": [
    "batch = dict()\n",
    "input_gen = prediction_utils.input_generator(bach, train_path, input_path, gt_tag)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        f_name, img, bps, I_a, gt, gt_cuts, seeds = next(input_gen)\n",
    "        graph = graph_utils.prims_initialize(img)\n",
    "        batch[f_name] = img, bps, I_a, gt, gt_cuts, seeds, graph\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'value' to a tensor and failed. Error: None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1fbc818ea750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchopin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChopinNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChopin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mchopin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreceptive_field_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#chopin.load_model(\"models/saved_model/Chopin/checkpoint\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchopin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/collin/PycharmProjects/Learned-Watershed-Segmentation/segmentation/models/ChopinNet.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, receptive_field_shape, learning_rate)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mgvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccum_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccum_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/collin/.local/lib/python2.7/site-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(self, delta, use_locking)\u001b[0m\n\u001b[1;32m    628\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0maddition\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \"\"\"\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/collin/.local/lib/python2.7/site-packages/tensorflow/python/ops/state_ops.pyc\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(ref, value, use_locking, name)\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ref_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     return gen_state_ops.assign_add(\n\u001b[0;32m--> 250\u001b[0;31m         ref, value, use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m    251\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/collin/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_state_ops.pyc\u001b[0m in \u001b[0;36massign_add\u001b[0;34m(ref, value, use_locking, name)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0muse_locking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_locking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 102\u001b[0;31m         \"AssignAdd\", ref=ref, value=value, use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/collin/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    526\u001b[0m               raise ValueError(\n\u001b[1;32m    527\u001b[0m                   \u001b[0;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                   (input_name, err))\n\u001b[0m\u001b[1;32m    529\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    530\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to convert 'value' to a tensor and failed. Error: None values not supported."
     ]
    }
   ],
   "source": [
    "chopin = ChopinNet.Chopin()\n",
    "chopin.build(receptive_field_shape, learning_rate=1e-5)\n",
    "#chopin.load_model(\"models/saved_model/Chopin/checkpoint\")\n",
    "chopin.initialize_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global_loss_timeline = []\n",
    "loss_timelines = dict()\n",
    "loss_file = open(\"data/train/chopin/global_loss.txt\", 'w')\n",
    "loss_file.write(\"f_name\\tepoch\\tloss\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for f_name, (img, bps, I_a, gt, gt_cuts, seeds, graph) in batch.iteritems():\n",
    "        print(\"Training on:\", f_name)\n",
    "        \n",
    "        foldername = os.path.join(train_path, \"chopin\", f_name)\n",
    "        start = time.time()\n",
    "        \n",
    "        loss, segmentations, cuts = chopin.train_on_image(img, I_a, gt_cuts, seeds, graph)\n",
    "        \n",
    "        if epoch % save_rate:\n",
    "            print(\"Saving Model\")\n",
    "            chopin.save_model(os.path.join(foldername, \"saved_models\", \"model\"), epoch)\n",
    "            chopin.save_model(\"models/saved_models/Chopin/chopin\", epoch)\n",
    "        \n",
    "        print(time.time() - start)\n",
    "        print(\"Loss: \", loss)\n",
    "        \n",
    "        plt.imsave(os.path.join(foldername, \"epoch_{}_bw.png\".format(epoch)), display_utils.view_boundaries(np.zeros_like(img), cuts))\n",
    "\n",
    "        mask = display_utils.transparent_mask(display_utils.view_boundaries(img, gt_cuts), segmentations, alpha=0.75)\n",
    "        plt.imsave(os.path.join(foldername, \"epoch_{}_overlay.png\".format(epoch)), mask)\n",
    "        \n",
    "        loss_file.write(f_name + \"\\t\" + str(epoch) + \"\\t\" + str(loss) + \"\\n\")\n",
    "        loss_file.flush()\n",
    "        \n",
    "        global_loss_timeline.append(loss)\n",
    "        \n",
    "        try:\n",
    "            loss_timelines[f_name].append(loss)\n",
    "        except KeyError:\n",
    "            loss_timelines[f_name] = [loss]\n",
    "            \n",
    "        plt.plot(loss_timelines[f_name])\n",
    "        plt.savefig(os.path.join(foldername, \"local_loss\"))\n",
    "        \n",
    "        plt.gcf().clear()\n",
    "            \n",
    "        plt.plot(global_loss_timeline)\n",
    "        plt.savefig(\"data/train/chopin/global_loss\")\n",
    "        \n",
    "        plt.gcf().clear()\n",
    "            \n",
    "loss_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
