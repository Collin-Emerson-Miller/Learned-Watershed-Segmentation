{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "from models import BachNet\n",
    "from models import ChopinNet\n",
    "\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from heapq import heappop as pop\n",
    "from heapq import heappush as push\n",
    "from utils import graph_utils\n",
    "from utils import display_utils\n",
    "from utils import prediction_utils\n",
    "from utils import preprocessing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"data/train\"\n",
    "test_path = \"data/test\"\n",
    "\n",
    "input_path = \"input\"\n",
    "output_path = \"output\"\n",
    "\n",
    "gt_tag = \"gt\"\n",
    "\n",
    "receptive_field_shape = (12, 12)\n",
    "n_epochs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bach = BachNet.BachNet()\n",
    "bach.build(receptive_field_shape, 1)\n",
    "bach.load_model('models/saved_models/Bach/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading image:  740_1450_gt\n",
      "Loading image:  740_1460\n",
      "Loading image:  740_1460\n",
      "Loading image:  740_1450\n",
      "Loading image:  740_1450\n",
      "Loading image:  740_1460_gt\n"
     ]
    }
   ],
   "source": [
    "batch = dict()\n",
    "input_gen = prediction_utils.input_generator(bach, train_path, input_path, gt_tag)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        f_name, img, bps, I_a, gt, gt_cuts, seeds = next(input_gen)\n",
    "        graph = graph_utils.prims_initialize(img)\n",
    "        batch[f_name] = img, bps, I_a, gt, gt_cuts, seeds, graph\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chopin = ChopinNet.Chopin()\n",
    "chopin.build(receptive_field_shape, learning_rate=1e-5)\n",
    "#chopin.load_model(\"models/saved_model/Chopin/checkpoint\")\n",
    "chopin.initialize_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.757458s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.038243s\n",
      "6.41453194618\n",
      "('Loss: ', 4.3006496429443359)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.790866s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.036516s\n",
      "6.37726688385\n",
      "('Loss: ', 14.371456146240234)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.761160s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.035656s\n",
      "6.31344604492\n",
      "('Loss: ', 3.523048534989357)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.862245s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.035404s\n",
      "6.37640595436\n",
      "('Loss: ', 1.1685757339000702)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.831154s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.036107s\n",
      "6.31888699532\n",
      "('Loss: ', 6.621144711971283)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.823822s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.036534s\n",
      "6.21793699265\n",
      "('Loss: ', 1.0308485627174377)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.757366s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.038165s\n",
      "6.27894806862\n",
      "('Loss: ', 44.735176086425781)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.751890s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.036821s\n",
      "6.11356019974\n",
      "('Loss: ', 1.1838516443967819)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.764888s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.036498s\n",
      "6.15739798546\n",
      "('Loss: ', 3.6260199546813965)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 6.070147s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.037277s\n",
      "6.43252110481\n",
      "('Loss: ', 0.77524378895759583)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.961958s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.035723s\n",
      "6.32901620865\n",
      "('Loss: ', 6.1738253831863403)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 6.122558s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.041264s\n",
      "6.58772110939\n",
      "('Loss: ', 0.44342809915542603)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 6.018631s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.190734s\n",
      "6.64679408073\n",
      "('Loss: ', 4.7154165506362915)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.804320s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.034629s\n",
      "6.16176605225\n",
      "('Loss: ', 0.032701969146728516)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.744350s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.036197s\n",
      "6.24313688278\n",
      "('Loss: ', 3.7731118202209473)\n",
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.669081s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.033107s\n",
      "6.03459191322\n",
      "('Loss: ', 0.63319188356399536)\n",
      "('Training on:', '740_1460')\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 5.797455s\n",
      "Starting gradient segmentation...\n",
      "Segmentation done: 0.035821s\n",
      "6.16393494606\n",
      "('Loss: ', 2.8307991027832031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/serl/anaconda2/lib/python2.7/site-packages/matplotlib/axis.py:1035: UserWarning: Unable to find pixel distance along axis for interval padding of ticks; assuming no interval padding needed.\n",
      "  warnings.warn(\"Unable to find pixel distance along axis \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training on:', '740_1450')\n",
      "Starting gradient segmentation...\n"
     ]
    }
   ],
   "source": [
    "global_loss_timeline = []\n",
    "loss_timelines = dict()\n",
    "loss_file = open(\"data/train/chopin/global_loss.txt\", 'w')\n",
    "loss_file.write(\"f_name\\tepoch\\tloss\\n\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for f_name, (img, bps, I_a, gt, gt_cuts, seeds, graph) in batch.iteritems():\n",
    "        print(\"Training on:\", f_name)\n",
    "        \n",
    "        foldername = os.path.join(train_path, \"chopin\", f_name)\n",
    "        start = time.time()\n",
    "        \n",
    "        loss, segmentations, cuts = chopin.train_on_image(img, I_a, gt_cuts, seeds, graph)\n",
    "        \n",
    "        if epoch % save_rate:\n",
    "            print(\"Saving Model\")\n",
    "            chopin.save_model(os.path.join(foldername, \"saved_models\", str(epoch)), epoch)\n",
    "            chopin.save_model(\"models/saved_models/Chopin/chopin\", epoch)\n",
    "        \n",
    "        print(time.time() - start)\n",
    "        print(\"Loss: \", loss)\n",
    "        \n",
    "        plt.imsave(os.path.join(foldername, \"epoch_{}_bw.png\".format(epoch)), display_utils.view_boundaries(np.zeros_like(img), cuts))\n",
    "\n",
    "        mask = display_utils.transparent_mask(display_utils.view_boundaries(img, gt_cuts), segmentations, alpha=0.75)\n",
    "        plt.imsave(os.path.join(foldername, \"epoch_{}_overlay.png\".format(epoch)), mask)\n",
    "        \n",
    "        loss_file.write(f_name + \"\\t\" + str(epoch) + \"\\t\" + str(loss) + \"\\n\")\n",
    "        loss_file.flush()\n",
    "        \n",
    "        global_loss_timeline.append(loss)\n",
    "        \n",
    "        try:\n",
    "            loss_timelines[f_name].append(loss)\n",
    "        except KeyError:\n",
    "            loss_timelines[f_name] = [loss]\n",
    "            \n",
    "        plt.plot(loss_timelines[f_name])\n",
    "        plt.savefig(os.path.join(foldername, \"local_loss\"))\n",
    "        \n",
    "        plt.gcf().clear()\n",
    "            \n",
    "        plt.plot(global_loss_timeline)\n",
    "        plt.savefig(\"data/train/chopin/global_loss\")\n",
    "        \n",
    "        plt.gcf().clear()\n",
    "            \n",
    "loss_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
